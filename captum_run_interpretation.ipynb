{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Interpret-flair"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Please note: this repository is not officially associated with [Flair](https://github.com/flairNLP/flair) nor with [Captum](https://github.com/pytorch/captum).\n",
        "This notebook shows an attempt at integrating [Captum](https://github.com/pytorch/captum) with a custom trained [Flair text-classifier](https://github.com/flairNLP/flair).\n",
        "As such, this approach should also be validated by outsiders.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The example model was trained on the [BBC dataset](https://www.kaggle.com/c/learn-ai-bbc/overview) and makes use of sentence-transformers' [xlm-r-100langs-bert-base-nli-mean-tokens model](https://huggingface.co/sentence-transformers/xlm-r-100langs-bert-base-nli-mean-tokens).\n",
        "The model has the following results:\n",
        "- F-score (micro) 0.964\n",
        "- F-score (macro) 0.9626\n",
        "- Accuracy 0.964"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from flair.models import TextClassifier\n",
        "from flair.data import Sentence\n",
        "from captum_interpretation_package.flair_model_wrapper import ModelWrapper\n",
        "from captum_interpretation_package.interpret_flair import interpret_sentence, visualize_attributions\n",
        "from captum.attr import LayerIntegratedGradients"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"./model/output/best-model.pt\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load the trained Flair classifier."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flair_model = TextClassifier.load(model_path)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to make use of Captum's [LayerIntegratedGradients method](https://captum.ai/api/layer.html#layer-integrated-gradients) we had to rework Flair's forward function. This is handled by the wrapper.\n",
        "The wrapper inherits functions of the Flair [text-classifier object](https://github.com/flairNLP/flair/blob/master/flair/models/text_classification_model.py) and allows us to calculate attributions with respect to a target class."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flair_model_wrapper = ModelWrapper(flair_model)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check out the underlying XLMRoberta model."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(flair_model_wrapper.model)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As described in the source code of [documentation of Captum](https://github.com/pytorch/captum/blob/master/captum/attr/_core/layer/layer_integrated_gradients.py):\n",
        "\n",
        "\n",
        "\"*Layer Integrated Gradients is a variant of Integrated Gradients that assigns\n",
        "an importance score to layer inputs or outputs, depending on whether we\n",
        "attribute to the former or to the latter one.*\"\n",
        "\n",
        "\n",
        "In this case, we are interested how the input embeddings of the model contribute to the output."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lig = LayerIntegratedGradients(flair_model_wrapper, flair_model_wrapper.model.embeddings)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test this, let's take the two paragraphs of an article about business by the Economist.\n",
        "\n",
        "\n",
        "[Which Japanese mogul will leave the biggest legacy?](https://www.economist.com/business/2020/11/07/which-japanese-mogul-will-leave-the-biggest-legacy)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"\"\"\n",
        "In the 1990s, when a youthful Son Masayoshi, a Japanese entrepreneur, was pursuing acquisitions in his home country, he sought advice from a banker eight years his junior called Mikitani Hiroshi. They shared a lot in common: both had studied in America (Mr Son at the University of California, Berkeley, Mr Mikitani at Harvard Business School); they had a common interest in the internet; and they were both baseball mad. In the decades since, both men have blazed past a stifling corporate hierarchy to become two of Japan’s leading tech billionaires. \n",
        "Mr Mikitani, who says in an interview that he did not even know the word “entrepreneur” when he enrolled at Harvard, pioneered e-commerce in Japan via Rakuten, which is now a sprawling tech conglomerate worth $14bn. Mr Son’s SoftBank, after spectacular investments in early internet stocks, muscled into Japan’s telecoms industry. They have both invested heavily in Silicon Valley. They also each own baseball teams named after birds of prey; the SoftBank Hawks and the Rakuten Golden Eagles.\n",
        "\"\"\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For convience, let's check the label dictionary to see which is 'business'.\n",
        "\n",
        "\n",
        "This can be useful if you have complex labels, or want to quickly reference labels used by the model."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(flair_model_wrapper.label_dictionary.get_item_for_index(1))\n",
        "\n",
        "target_label = flair_model_wrapper.label_dictionary.get_item_for_index(1)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also create an empty list to store our attribitions results in order to visualize them using Captum."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "visualization_list = []"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run the Layer Integrated Gradient method on the two paragraphs, and determine what drives the prediction.\n",
        "\n",
        "As an additional note, the number of steps & the estimation method can have an impact on the attribution.\n",
        "\n",
        "This [tutorial](https://colab.research.google.com/drive/1pgAbzUF2SzF0BdFtGpJbZPWUOhFxT2NZ#scrollTo=sO0Wr7j6TPOR) even uses 7000 steps!"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "readable_tokens, word_attributions, delta = interpret_sentence(flair_model_wrapper,\n",
        "                                                                lig,\n",
        "                                                                sentence,\n",
        "                                                                target_label,\n",
        "                                                                visualization_list,\n",
        "                                                                n_steps=500,\n",
        "                                                                estimation_method=\"gausslegendre\",\n",
        "                                                                internal_batch_size=3)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_attributions(visualization_list)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokenizer used by your model will have an impact how the original text is displayed. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also see the importance scores of each token."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_scores = word_attributions.detach().numpy()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ordered_lig = [(readable_tokens[i], word_scores[i]) for i in np.argsort(word_scores)][::-1]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ordered_lig"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Contributions & Suggestions\n",
        "\n",
        "[Pull requests](https://github.com/robinvanschaik/interpret-flair/pulls) and [issues](https://github.com/robinvanschaik/interpret-flair/issues) are welcome! \n",
        "\n",
        "\n",
        "Check out this [discussion](https://github.com/flairNLP/flair/issues/1504) regarding explainable AI & Flair integration."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Authors\n",
        "\n",
        "* [Robin van Schaik](https://github.com/robinvanschaik)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Acknowledgements\n",
        "\n",
        "* [Flair](https://github.com/flairNLP/flair) for the text classification training framework.\n",
        "* [Sentence transformers](https://github.com/UKPLab/sentence-transformers) for great sentence-level language models that can be used in Flair.\n",
        "* [Huggingface](https://github.com/huggingface/transformers) for a large collection of language models that can be used in Flair.\n",
        "* [Captum](https://github.com/pytorch/captum) for providing the model interpretation framework.\n",
        "* This [tutorial](https://captum.ai/tutorials/Bert_SQUAD_Interpret) by the Captum team helped me to get started.\n",
        "* This [discussion](https://github.com/pytorch/captum/issues/414) regarding Captum & XLM type models was also very insightful."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 0
}